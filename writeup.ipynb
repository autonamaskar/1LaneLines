{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"writeup.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wrjwPlIJ_oz8"},"source":["# **Finding Lane Lines on the Road**  \n","  \n","One of the most basic aspects of driving a car (safely) is to make sure the car is within a lane. We as a human driver do it almost instictively or say involuntarily by constantly correcting to make sure the vehicle doesn't veer off into another vehicles path. What we humans do visually can be replicated in an autonomous vehicle using a series of Computer Vision techniques. The objective of this implementation and the process pipeline required to achieve this is explained below.  \n","  \n","## Objective  \n","  \n","The objective of this project is to implement a set of functions which combine together to identify lane lines on a road. When presented with a video stream from a front view camera, this implementation should identify the left and right lane markings and highlight them as a thick red line extending from the front of the vehicle into the plane of the image away from it as shown below.  \n","  \n","<figure>\n"," <img src=\"examples/line-segments-example.jpg\" width=\"380\" alt=\"Combined Image\" />\n"," <figcaption>\n"," <p></p> \n"," <p style=\"text-align: center;\"> Your output should look something like this (above) after detecting line segments using the helper functions below </p> \n"," </figcaption>\n","</figure>\n"," <p></p> \n","<figure>\n"," <img src=\"examples/laneLines_thirdPass.jpg\" width=\"380\" alt=\"Combined Image\" />\n"," <figcaption>\n"," <p></p> \n"," <p style=\"text-align: center;\"> Your goal is to connect/average/extrapolate line segments to get output like this</p> \n"," </figcaption>\n","</figure>\n","  \n","## Workspace  \n","  \n","For this nanodegree, while working on python based projects, I've decided to use Google Colab as my workspace. Gogogle Colab really freed me to go to the hastle of installing the tool chain which often gives installation nightmare. Few of the advantages of using Colab I have mentioned below.\n","\n","1. Most python libraries required for this nano degree are readily available on Colab and adding new libraries is very easy as well  \n","2. Gives me flexibility of carrying my workspace whereever I want  \n","3. Can easily fire up GPU and TPU supported instances and speed up some of the training process  \n","4. Can mount my google Drive on Colab and have a copy of my files on the cloud \n","5. Can commit my work to github directly from colab  \n","  \n","## Lane Detection Pipeline  \n","The input for the lane detection task , as mentioned earlier, is a video stream from the front view camera. The video stream in reality is only a continuous stream of individual image frames. So, our lane detection effectively starts by extracting each frame from a video. Once we have the individual frame, the processing pipeline follows  \n","  \n","1. Convert the color image to gray scale image \n","2. Applying Gaussian blur  \n","3. Detect edges in the image using Canny Edge Detector  \n","4. Mask and retain only the region of interest from the entire image  \n","5. Apply Hough Transform on the masked edge features to identify lines  \n","6. Identify the left and right lane lines from their slopes  \n","7. From all the detected left and right lanes, get the average left line's slope and intercept and the right line's slope and intercept  \n","8. Use the average slope to draw an extrapolated line in the region of interest \n","9. Overlay the extrapolated line on the original image  \n","10. Run the function on the video which is nothing but the set of images\n","\n","\n","### Orginial images ### \n","\n","<figure>\n"," <img src=\"test_images_output/Original.png\" width=\"380\" alt=\"Example Image\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> Orginal example image having solid white curve on the right and dotted white curve on the left. </p> \n"," </figcaption>\n","</figure>\n","\n","\n","### Grayscale and Blurring ### \n","The basic idea behind lane line detection is the identification of the edges that constitute the lane. Although edge detection can be done on a RGB image, the effective edge is determined by the channel with the maximum gradient. So, for efficient edge detection, the first task is to convert to **gray** scale image. Converting the gray scale makes the image into a 8 bit image which can take 256 value (0, 255) -- **0 (black), 255(white)**\n"," \n"," <figure>\n"," <img src=\"test_images_output/Gray.png\" width=\"380\" alt=\"Gray Image\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> Converting to Gray scale. </p> \n"," </figcaption>\n","</figure>\n","\n"," <p></p>\n","Blurring is another important technique which helps reduce high frequency content in the image and hence reduces noise and helps in better edge extraction. There are several blurring methods available in openCV. The most commonly used method is the Gaussian Blur. In this method, a ** M x N ** kernel is convolved with the input image and the intensity of each pixel is modified to the weighted average of the intensities of the pixels in the kernel. \n","\n"," <figure>\n"," <img src=\"test_images_output/Gaussian.png\" width=\"380\" alt=\"Gaussian blur Image\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> Applying Gaussian Blur. </p> \n"," </figcaption>\n","</figure>\n","\n","\n","\n","### Canny Edge Detection  \n","Canny edge detection is a technique used to identify gradients of any orientation in an image that helps us extract the structural information in an image. \n","\n","- First the Canny edge algorithm does the Gaussian Smoothing, which is to suppress noise and spurious gradients by averaging.\n","- Second computes to gradient and detects the strong edge.\n","- It is recommended to keep the ratio of low_threshold to high threshold to 1:2 or 1:3 . Algorthim detects the strong edge pixels above the high_threshold and rejects pixels below the low_threshold. pixels in between are included as long as they are connected to strong edges. Output edges is a binary image with white pixels tracing out the detected edges and black pixels everywhere else.\n","\n"," <figure>\n"," <img src=\"test_images_output/Canny_edge.png\" width=\"380\" alt=\"Canny edge Image\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> Compute to gradient by applying Canny edge. </p> \n"," </figcaption>\n","</figure>\n","\n"," \n"," ### Region of Interest\n","  \n"," Since we are only interested in the lane lines, the region of the image that is not immediately in front of the vehicle, like the cars in the adjascent lane, the barriers at the extreme left and right can all be neglected. This is achieved by masking the blurred image with a polygon that covers only the region of interest. This polygon is defined by four vertices [bottom_left, top_left, top_right, bottom_right]. Once we define the vertices, we mask the area of interest to suppress the noise by blanking areas outside region of interest.\n"," \n"," \n"," <figure>\n"," <img src=\"test_images_output/ROI_masked.png\" width=\"380\" alt=\"Maksed ROI\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> Masking the region of interest. </p> \n"," </figcaption>\n","</figure>\n"," \n"," Two different polygons were used for the two different sets of videos in the project. The solidWhiteRIght and the solidYellowLeft videos have the same mask where as the challenge video has a different mask. \n","    \n","### Hough Transform\n","\n","Given a set of edge points in a binary image, Hough transformation is used to find a line that passes through all these points. \n","A line in a cartesian space is represented by its slope $m$ and intercept $b$ as \n","$y=mx+b$\n","\n","The same line when transformed in the $m-b$ space will be a point as shown in the image below\n","\n"," <figure>\n"," <img src=\"test_images_output/hough-cartesian.png\" width=\"380\" alt=\"Hough Vs Cartesian\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> Line in a image space will be a point in the Hough space. </p> \n"," </figcaption>\n","</figure>\n","\n","Where this representation fails is when the line is vertical with an infinite slope. To over come this, the $\\rho-\\theta$ space is used. The same line in $\\rho-\\theta$  space can be represented as \n","$\\rho = x*cos(\\theta) + y*sin(\\theta)$\n","\n","Curves generated by collinear points in the image space intersect at common $(\\rho,\\theta)$ in the Hough transform space. The more curves intersect at a point, the more vote the line gets. Thus by using a **threshold** number of votes per line, we can select prominent lines in an image. \n","\n"," <figure>\n"," <img src=\"test_images_output/hough-thetha.png\" width=\"380\" alt=\"Hough Vs Cartesian Polar\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> Line in a image space's polar coordinate will be a point in the Hough space. </p> \n"," </figcaption>\n","</figure>\n","\n","### Identify the left and right line and get the average slope\n","\n","Once the lines are obtained from the Hough transform, we can pass the line end points to a first order polynomial fit function in numpy to get the slope and y-intercept of the fitted line. We can identify the left and right line by checking their slopes. The line with a negative slope is the left line and the one with the positive slope is the right line. This is due to the fact that for an image the (0,0) is at the top left corner of the image. \n","\n","The average slope of these line making up the left and right lines can be obtained by taking a weighted average of all the individual line slopes. The slopes are weighted based on the length of the line and hence the average slope is influenced more by the longer lines than the shorter ones. \n","\n"," <figure>\n"," <img src=\"test_images_output/Hough.png\" width=\"380\" alt=\"Hough\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> After performing Hough transform. </p> \n"," </figcaption>\n","</figure>\n","\n","\n","### Draw and overlay the extrapolated lane lines on the image\n","\n","Using the averaged line slopes and intercepts , we can find the x co-ordinate of the start and end of the line. These lines are drawn on the original image with a set transparency to highlight the identified lane lines as shown below\n","\n"," <figure>\n"," <img src=\"test_images_output/Final.png\" width=\"380\" alt=\"Final\" />\n"," <figcaption>\n"," <p style=\"text-align: center;\"> After overlaying the image to the orginal image.</p> \n"," </figcaption>\n","</figure>\n","    \n","## Limitations \n","    \n","There are several limitations to this implementation. To list a few\n","    \n","1. Even with the simpler videos, the lane line flickers when there is some noise in the image. This stems from the fact that the average slope of the lane line is computed fresh for every frame and if edges with different slopes are detected between consecutive frames, the lane line is redrawn and appears as a flicker. This can be eliminated by keeping a history of the slopes of the previous few frames and taking an average of that would result in a more stable lane line. \n","2. In the challenge video, when the roads switches from asphalt to concrete, the intersection is identified as an edge. To avoid this, in the current implementation every edge is checked for its angle and if the edge is less than $25^0$ it is not drawn. As a result the portion of the video that switches from asphalt to concrete does not have the lane marking. Even after thresholding the grayscale image, the problem persisted. \n","3. All lane lines are drawn as straignt lines. Hence the curvature of the road is not captured. A polynomial curve fitting needs to be implemented to be able to draw curved lane marking. "]}]}
